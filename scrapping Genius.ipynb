{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib, os, urllib.request\n",
    "import re\n",
    "import requests\n",
    "from mutagen.mp3 import MP3\n",
    "from mutagen.id3 import ID3, TIT2\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "\n",
    "lyricsPath = os.getcwd()+'\\\\songs_lyrics'\n",
    "if not os.path.exists(lyricsPath):\n",
    "    os.makedirs(lyricsPath)\n",
    "lyricsPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def urlify(s):\n",
    "\n",
    "    # Remove all non-word characters (everything except numbers and letters)\n",
    "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
    "\n",
    "    # Replace all runs of whitespace with a single dash\n",
    "    s = re.sub(r\"\\s+\", '-', s)\n",
    "\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateUrls():\n",
    "    print(\"----------- generating urls -----------\")\n",
    "    path = 'D:\\MUSIC\\sounds\\RAP\\Eminem'\n",
    "    files = []\n",
    "    urls = []\n",
    "    # r=root, d=directories, f = files\n",
    "    for r, d, f in os.walk(path):\n",
    "        for file in f:\n",
    "            if '.mp3' in file :\n",
    "                files.append(os.path.join(r, file))\n",
    "                \n",
    "    for f in files:\n",
    "        try :\n",
    "            tags = ID3(os.path.join(r,f))\n",
    "            if 'TIT2' in tags:\n",
    "                #songName = urlify(tags['TIT2'].text[0])\n",
    "                songName = tags['TIT2'].text[0]\n",
    "                songName = re.sub(r\"feat+.([^\\w]|[\\w])*\",\"\",songName,flags=re.IGNORECASE)\n",
    "                songName = urlify(songName)\n",
    "                songName = songName.replace(\"'\",'')\n",
    "                if songName[-1]=='-':\n",
    "                    songName = songName[:-1]\n",
    "                url = \"https://genius.com/eminem-\"+songName+\"-lyrics\"\n",
    "                urls.append(url)\n",
    "        except :\n",
    "            tags = ID3()\n",
    "    urls = list(dict.fromkeys(urls))\n",
    "    urls\n",
    "    urlsFile = open(os.path.expanduser(r\"urls.csv\"),\"wb\")\n",
    "    for url in urls:\n",
    "        urlsFile.write(bytes(url+\"\\n\",encoding=\"ascii\", errors='ignore'))\n",
    "    urlsFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateWorkingAndNotWorkingUrlsFile():\n",
    "    print(\"----------- generating files -----------\")\n",
    "    urls = []\n",
    "    notWorkingUrls = []\n",
    "    workingURLS =[]\n",
    "    with open(os.path.expanduser(r\"urls.csv\"),\"r\") as urlsFile:\n",
    "       # urls = csv.reader(urlsFile, delimiter=' ', quotechar='|')\n",
    "        for url in urlsFile:\n",
    "            urls.append(url)\n",
    "    urlsFile.close()\n",
    "\n",
    "    hdr = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36',\n",
    "             'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "             'Referer': 'https://cssspritegenerator.com',\n",
    "             'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "             'Accept-Encoding': 'none',\n",
    "             'Accept-Language': 'en-US,en;q=0.8',\n",
    "             'Connection': 'keep-alive'}\n",
    "    for url in urls:\n",
    "        try:\n",
    "            req=urllib.request.Request(url,headers=hdr)\n",
    "            thePage = urllib.request.urlopen(req)\n",
    "            soup = BeautifulSoup(thePage, \"html.parser\")\n",
    "            title = soup.find('div',class_='header_with_cover_art-primary_info').h1.text\n",
    "            title = re.sub(r'/',\"-\",title)\n",
    "            workingURLS.append(url)\n",
    "        except urllib.error.HTTPError as err:\n",
    "            if err.code == 404:\n",
    "                notWorkingUrls.append(url)\n",
    "            else:\n",
    "                raise\n",
    "    workingURLS = list(dict.fromkeys(workingURLS))\n",
    "    notWorkingUrls = list(dict.fromkeys(notWorkingUrls))\n",
    "    urls = list(dict.fromkeys(urls))\n",
    "    notUrlsFile = open(os.path.expanduser(r\"not-working-urls.csv\"),\"wb\")\n",
    "    for url in notWorkingUrls :\n",
    "        urls.remove(url)\n",
    "        notUrlsFile.write(bytes(url,encoding=\"ascii\", errors='ignore'))\n",
    "    notUrlsFile.close()\n",
    "    urlsFile = open(os.path.expanduser(r\"working-urls.csv\"),\"wb\")\n",
    "    for url in workingURLS:\n",
    "        urlsFile.write(bytes(url,encoding=\"ascii\", errors='ignore'))\n",
    "    urlsFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWorkingUrls():\n",
    "    urls = []\n",
    "    i=0\n",
    "    with open(os.path.expanduser(r\"working-urls.csv\"),\"r\") as urlsFile:\n",
    "        for url in urlsFile:\n",
    "            urls.append(url)\n",
    "    urlsFile.close()\n",
    "    urls.sort()\n",
    "    urls = list(dict.fromkeys(urls))\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getExistingUrls():\n",
    "    existingUrls = []\n",
    "    if os.path.isfile(lyricsPath+r'\\existing-urls.csv'):\n",
    "        with open(os.path.expanduser(lyricsPath+r'\\existing-urls.csv'),'r') as existingUrlsFile:\n",
    "            for url in existingUrlsFile:\n",
    "                url = re.sub(r'.*url : ', '',url)\n",
    "                existingUrls.append(url)\n",
    "        existingUrlsFile.close()\n",
    "        existingUrls.sort()\n",
    "        return existingUrls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scrappingWorkingUrls():\n",
    "    print(\"----------- scrapping urls -----------\")\n",
    "    pathExistingUrlsFile= lyricsPath+r'\\existing-urls.csv'\n",
    "    pathExistingUrlsFile\n",
    "    if os.path.isfile(pathExistingUrlsFile):\n",
    "        existingUrlsFile = open(os.path.expanduser(pathExistingUrlsFile),'ab')\n",
    "    else:\n",
    "        existingUrlsFile = open(os.path.expanduser(pathExistingUrlsFile),'wb')\n",
    "\n",
    "    hdr = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36',\n",
    "             'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "             'Referer': 'https://cssspritegenerator.com',\n",
    "             'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "             'Accept-Encoding': 'none',\n",
    "             'Accept-Language': 'en-US,en;q=0.8',\n",
    "             'Connection': 'keep-alive'}\n",
    "    urls = getWorkingUrls()\n",
    "    existingUrls = getExistingUrls()\n",
    "    for url in urls :\n",
    "        if  not url in existingUrls:\n",
    "            try:\n",
    "                req=urllib.request.Request(url,headers=hdr)\n",
    "                thePage = urllib.request.urlopen(req)\n",
    "                soup = BeautifulSoup(thePage, \"lxml\")\n",
    "                lyrics = soup.find('div', class_=\"lyrics\").p.text\n",
    "                title = soup.find('div',class_='header_with_cover_art-primary_info').h1.text\n",
    "                title = re.sub(r'/',\"-\",title)\n",
    "                title = re.sub(r'[^\\w\\s]|[\\-\\(\\)\\']','',title)\n",
    "                artist = soup.find('div',class_='header_with_cover_art-primary_info').h2.text.lstrip().rstrip()\n",
    "                if not os.path.exists(lyricsPath+\"\\\\\"+title+\".txt\"):\n",
    "                    songFile = open(os.path.expanduser(lyricsPath+\"\\\\\"+title+\".txt\"),\"wb\")\n",
    "                    songFile.write(bytes(\"Title : \"+title+\"\\nArtist : \"+artist+\"\\nLyrics :\\n\"+lyrics,encoding=\"ascii\", errors='ignore'))\n",
    "                    songFile.close()\n",
    "                    existingUrlsFile.write(bytes(\"Title : \"+title+\" , url : \"+url,encoding=\"ascii\", errors='ignore'))\n",
    "                    print(urls[urls.index(url)])\n",
    "            except urllib.error.HTTPError as err:\n",
    "                raise\n",
    "    existingUrlsFile.close()\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(r'urls.csv'):\n",
    "    generateUrls()\n",
    "generateWorkingAndNotWorkingUrlsFile()\n",
    "scrappingWorkingUrls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
