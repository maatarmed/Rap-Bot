{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is more for fun than a puzzle\n",
    "# \n",
    "# Find some text, you could download a book from project gutenberg, or r you could dump \n",
    "# all of the code in this project into one text file with 'cat ../../**/*.py > code.txt'\n",
    "# \n",
    "# Next run this character-based GRU with char-gen.py some-text-file.txt\n",
    "# If you are on a GPU you should use CuDNNGRU in place of GRU\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM, SimpleRNN\n",
    "from keras.layers import CuDNNGRU, GRU\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "#import wandb\n",
    "#from wandb.keras import WandbCallback\n",
    "import argparse\n",
    "import pymongo\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from numpy import loadtxt\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parser = argparse.ArgumentParser()\n",
    "#parser.add_argument(\"text\", type=str)\n",
    "\n",
    "#args = parser.parse_args()\n",
    "\n",
    "\n",
    "hidden_nodes = 128\n",
    "batch_size = 256\n",
    "file = \"Lyrics.txt\"\n",
    "maxlen = 200\n",
    "step = 3\n",
    "\n",
    "# Only load first 100k charcters because we're not using memory efficiently\n",
    "text = io.open(file, encoding='utf-8').read()[:10000]\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# build a sequence for every <step>-th character in the text\n",
    "\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "\n",
    "# build up one-hot encoded input x and output y where x is a character\n",
    "# in the text y is the next character in the text\n",
    "\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(GRU(hidden_nodes, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"rmsprop\")\n",
    "#model.add(GRU(hidden_nodes, input_shape=(256, len(chars))))\n",
    "model.load_weights(\"weights/weights-improvement.hdf5\")\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "class SampleText(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "        for diversity in [0.5, 1.2]:\n",
    "            print()\n",
    "            print('----- diversity:', diversity)\n",
    "\n",
    "            generated = ''\n",
    "            sentence = text[start_index: start_index + maxlen]\n",
    "            generated += sentence\n",
    "            print('----- Generating with seed: \"' + sentence + '\"')\n",
    "            sys.stdout.write(generated)\n",
    "\n",
    "            for i in range(200):\n",
    "                x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "                for t, char in enumerate(sentence):\n",
    "                    x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "                preds = model.predict(x_pred, verbose=0)[0]\n",
    "                next_index = sample(preds, diversity)\n",
    "                next_char = indices_char[next_index]\n",
    "\n",
    "                generated += next_char\n",
    "                sentence = sentence[1:] + next_char\n",
    "\n",
    "                sys.stdout.write(next_char)\n",
    "                sys.stdout.flush()\n",
    "            print()\n",
    "            \n",
    "filepath=\"weights/weights-improvement.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n",
    "                             save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "#callbacks_list = [checkpoint]\n",
    "\n",
    "model.fit(x, y, batch_size=batch_size,\n",
    "          epochs=10, callbacks=[SampleText(),checkpoint])#, WandbCallback()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:scrapping_project] *",
   "language": "python",
   "name": "conda-env-scrapping_project-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
